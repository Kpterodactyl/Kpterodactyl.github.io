
<p>Spark注重建立良好的生态系统，不仅支持多种外部文件存储系统，而且提供了多种运行模式。部署在单台机器上时，既可以用本地（Local）模式运行，也可以使用伪分布式模式来运行；当以分布式集群部署是，可以根据自己集群的实际情况选择独立（standalone）运行模式来运行、YARN运行模式、还是Mesos运行模式。   <br />
Spark虽然支持多种运行模式，但Spark应用程序的运行架构基本由三部分组成，包括</p>
<ol>
  <li>SparkContext（驱动程序）</li>
  <li>ClusterManager（集群资源管理程序）</li>
  <li>Executor（任务执行进程） <br />
 <img src="http://i.imgur.com/RHzn6FF.png" alt="" />
    <h2 id="各部分功能详解">各部分功能详解</h2>
    <p><strong>Driver</strong>是专门用来提交Spark程序的机器： <br />
这台机器一般一定和Spark Cluster在同样的网络环境中（Driver和Executor通信频繁），且其配置和普通的Worker一致。  Application（各种依赖的外部资源，例如*.so File)，使用<code class="highlighter-rouge">spark-submit</code>去运行各种程序（可以配置运行时的各种参数，例如memory cores…),实例生产环境下写Shell脚本自动化配置和提交程序。  <br />
Dirver的核心是<strong>SparkContext</strong>其作用为：  <br />
创建DAGScheduler、TaskScheduler、SchedulerBackend，在实例化的过程中Register当前程序给Master，Master接收注册，如果没有问题，Master会为当前程序分配AppId并分配计算资源。
一般情况下，当通过action触发Job时SparkContex会通过DAGScheduler来把Job中的RDD构成的DAG划分为不同的Stage，每个Stage内部是一系列业务逻辑相同但处理数据不同的Tasks，构成了TaskSet。 TaskScheduler和SchedulerBackend负责具体Task的运行（遵循数据本地性）。</p>
  </li>
</ol>

<p><strong>Spark Cluster</strong> 中<strong>Master</strong>：    <br />
接收用户提交的程序并发送指令给Worker为当前程序分配计算资源，每个Worker所在节点默认为当前程序分配一个Executor，在Executor中通过线程池并发执行。    <br />
Master通知Worker按照要求启动Exectuor。</p>

<p><strong>Worker Node</strong>中Worker进程，通过一个Proxy为ExecutorRunner的对象实例来远程启动ExecutorBackend进行； <br />
<strong>ExecutorBacken</strong>进程里面有Executor，实际在工作时候会通过TaskRunner来封装Task，然后从ThrePool中获取一条线程执行Task，执行完后线程被回收复用。 最后一个StageTask称为ResultTask，产生Job结果，其它前面的Stage中的Task都是ShuffleMapTask，为下一阶段的Stage做数据准备，相当于MapReduce中的mapper。</p>

<p>整个Spark程序的运行，就是DAGScheduler把Job划分成不同的Stage，提交TaskSet给TaskScheduler，进而提交给Executor执行（符合数据本地性），每个Task会计算RDD中的一个Partition，基于Partition来具体执行我们定义的一系列同一个Stage内部的函数，依此类推直到整个程序完成。
<img src="http://i.imgur.com/Molmr7u.png" alt="" /></p>

