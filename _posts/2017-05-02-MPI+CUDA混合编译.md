---
layout:     post
title:      MPI+CUDA混合编译
subtitle:   结合实例详解MPI主从模式
date:       2017-04-28  19:03:08 +0800
author:     Memory
header-img: img/post-bg-js-module.jpg
catalog: MPI
tags:
    - CUDA
    - MPI
    - gcc编译
---


努力了很久写完了Apriori关联聚类的CUDA程序，为了适应更大数量级的文本文件，我要开始把CUDA_Apriori算法再用MPI并行。因为是文本关联聚类，文件的分割会破坏原本的关联关系，所以实现并行的难度就比较高了。      
暂时的可行方法如下：    
K为当前的频繁度，当K>1时    
![](http://i.imgur.com/WAqiUqK.png)     
其中Lk为K阶频繁项集，各从节点的LK加和得到CGk（全局频繁项候选集），在各个从节点只做频繁项的计数操作，不计算支持度的计算。  
那么实现这一功能的第一步就是文本的分割和传输：   
1. MPI初始化等一系列操作      
2. 读取文本文件，根据行和每行中的最大元素数量确定winSize大小      
3. 创建窗口     
4. `myid == 0` 为主节点 ，进行文本文件的读取，得到transactions[]数组，对远地窗口进行数据写操作 `MPI_PUT(transactions, lines*30, MPI_UNSIGNED_CHAR, 0, 0, lines*30, MPI_INT32_T, window)`    
5. 其它数据以广播的方式发送到各从节点 `MPI_Send( &lines,1, MPI_INT32_T, i, 101, MPI_COMM_WORLD)`   
6. `myid != 0` 为各从节点，   `MPI_Recv(&lines,1, MPI_INT32_T, 0, 101, MPI_COMM_WORLD, &status_t)` 接受数据   
7.  `MPI_Get(transactions, offset+10, MPI_INT32_T, 0, start,  offset+10 ,MPI_INT32_T, window)` 获得数据窗口的数据  
8.  各个从节点开始各自的计算   
9.  结束MPI程序的运行      

**需要注意的地方**    
- MPI_Send和MPI_Recv的顺序要对应，消息代号的顺序从小到大   
- 注意不要出现动态分配的内存释放多次

## MPI+CUDA混合编译 ##   
MPI和CUDA可以放在一个文件中，该文件可以通过nvcc把.cu文件编译成.o文件，用nvcc链接成可执行程序,链接由mpiexec运行，如：
 {% highlight ruby %}
编译
nvcc -o main.o -c main.cu -I ~/mpich-3.2/include/ -L ~/mpich-3.2/lib/ -lmpi
链接
nvcc -o main main.o support.o -L ~/mpich-3.2/lib/ -lmpi
运行
mpiexec -np 2 ./main
{% endhighlight %}



    
